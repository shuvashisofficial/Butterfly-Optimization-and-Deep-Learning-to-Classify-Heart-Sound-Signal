{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T16:59:58.496135Z",
     "iopub.status.busy": "2024-03-23T16:59:58.495786Z",
     "iopub.status.idle": "2024-03-23T16:59:58.504929Z",
     "shell.execute_reply": "2024-03-23T16:59:58.503845Z",
     "shell.execute_reply.started": "2024-03-23T16:59:58.496110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import pydot\n",
    "import kymatio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from termcolor import colored\n",
    "import matplotlib.pyplot as plt \n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "from kymatio import Scattering1D\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:00.547182Z",
     "iopub.status.busy": "2024-03-23T17:00:00.546814Z",
     "iopub.status.idle": "2024-03-23T17:00:00.555528Z",
     "shell.execute_reply": "2024-03-23T17:00:00.554326Z",
     "shell.execute_reply.started": "2024-03-23T17:00:00.547153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, Activation, Input, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf\n",
    "#import tensorflow_io as tfio\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CallBacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:02.803234Z",
     "iopub.status.busy": "2024-03-23T17:00:02.802425Z",
     "iopub.status.idle": "2024-03-23T17:00:02.811682Z",
     "shell.execute_reply": "2024-03-23T17:00:02.810664Z",
     "shell.execute_reply.started": "2024-03-23T17:00:02.803189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "model_file_path = \"best_model_file.hdf5\"\n",
    "checkpoint = ModelCheckpoint(model_file_path, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=5,  # <-- Corrected value\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger('Model.log')\n",
    "\n",
    "# Define a learning rate schedule\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    end_learning_rate=0.000001,\n",
    "    power=0.5,\n",
    "    cycle=False\n",
    ")\n",
    "\n",
    "\n",
    "callbacks = [earlystop, csv_logger, reduce_lr, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:08.205447Z",
     "iopub.status.busy": "2024-03-23T17:00:08.205082Z",
     "iopub.status.idle": "2024-03-23T17:00:08.211942Z",
     "shell.execute_reply": "2024-03-23T17:00:08.211048Z",
     "shell.execute_reply.started": "2024-03-23T17:00:08.205419Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.listdir(r'D:\\MIET_HeartSound\\Dataset\\Dataset2\\heart_sound')\n",
    "data = os.path.join(r'D:\\MIET_HeartSound\\Dataset\\Dataset2\\heart_sound')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:12.706675Z",
     "iopub.status.busy": "2024-03-23T17:00:12.705841Z",
     "iopub.status.idle": "2024-03-23T17:00:12.711017Z",
     "shell.execute_reply": "2024-03-23T17:00:12.709894Z",
     "shell.execute_reply.started": "2024-03-23T17:00:12.706637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dir_path = os.path.join(data, 'train')\n",
    "valid_dir_path = os.path.join(data, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:14.420325Z",
     "iopub.status.busy": "2024-03-23T17:00:14.419624Z",
     "iopub.status.idle": "2024-03-23T17:00:14.424562Z",
     "shell.execute_reply": "2024-03-23T17:00:14.423681Z",
     "shell.execute_reply.started": "2024-03-23T17:00:14.420293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "healthy = os.path.join(train_dir_path, 'healthy', 'a0007.wav')\n",
    "unhealthy = os.path.join(train_dir_path, 'unhealthy', 'a0002.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:16.172760Z",
     "iopub.status.busy": "2024-03-23T17:00:16.172401Z",
     "iopub.status.idle": "2024-03-23T17:00:16.179007Z",
     "shell.execute_reply": "2024-03-23T17:00:16.178023Z",
     "shell.execute_reply.started": "2024-03-23T17:00:16.172731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_wav_16k_mono(filename):\n",
    "    # Load encoded wav file\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    # Decode wav (tensors by channels)\n",
    "    wav, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1)\n",
    "    wav = wav[:10000]\n",
    "    print(wav, sample_rate)\n",
    "    # Removes trailing axis\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    print(wav, sample_rate)\n",
    "    \n",
    "    # Goes from 44100Hz to 16000hz - amplitude of the audio signal\n",
    "#     wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:17.939601Z",
     "iopub.status.busy": "2024-03-23T17:00:17.939245Z",
     "iopub.status.idle": "2024-03-23T17:00:18.196019Z",
     "shell.execute_reply": "2024-03-23T17:00:18.195041Z",
     "shell.execute_reply.started": "2024-03-23T17:00:17.939574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wave = load_wav_16k_mono(healthy)\n",
    "nwave = load_wav_16k_mono(unhealthy)\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.plot(nwave, color='black')\n",
    "plt.plot(wave, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:20.822612Z",
     "iopub.status.busy": "2024-03-23T17:00:20.822236Z",
     "iopub.status.idle": "2024-03-23T17:00:21.680243Z",
     "shell.execute_reply": "2024-03-23T17:00:21.679071Z",
     "shell.execute_reply.started": "2024-03-23T17:00:20.822582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "healthy_heart_train = tf.data.Dataset.list_files(train_dir_path+'/healthy'+'/*.wav')\n",
    "unhealthy_heart_train = tf.data.Dataset.list_files(train_dir_path+'/unhealthy'+'/*.wav')\n",
    "#healthy_heart_valid = tf.data.Dataset.list_files(valid_dir_path+'/healthy'+'/*.wav')\n",
    "#unhealthy_heart_valid = tf.data.Dataset.list_files(valid_dir_path+'/unhealthy'+'/*.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:23.472903Z",
     "iopub.status.busy": "2024-03-23T17:00:23.471931Z",
     "iopub.status.idle": "2024-03-23T17:00:23.489818Z",
     "shell.execute_reply": "2024-03-23T17:00:23.488665Z",
     "shell.execute_reply.started": "2024-03-23T17:00:23.472859Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hhl_train = tf.data.Dataset.zip((healthy_heart_train, tf.data.Dataset.from_tensor_slices(tf.ones(len(healthy_heart_train)))))\n",
    "uhl_train = tf.data.Dataset.zip((unhealthy_heart_train, tf.data.Dataset.from_tensor_slices(tf.zeros(len(unhealthy_heart_train)))))\n",
    "train_data = hhl_train.concatenate(uhl_train)\n",
    "\n",
    "#hhl_valid = tf.data.Dataset.zip((healthy_heart_valid, tf.data.Dataset.from_tensor_slices(tf.ones(len(healthy_heart_valid)))))\n",
    "#uhl_valid = tf.data.Dataset.zip((unhealthy_heart_valid, tf.data.Dataset.from_tensor_slices(tf.zeros(len(unhealthy_heart_valid)))))\n",
    "#valid_data = hhl_valid.concatenate(uhl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:25.500184Z",
     "iopub.status.busy": "2024-03-23T17:00:25.499462Z",
     "iopub.status.idle": "2024-03-23T17:00:31.887874Z",
     "shell.execute_reply": "2024-03-23T17:00:31.887050Z",
     "shell.execute_reply.started": "2024-03-23T17:00:25.500135Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train_data = train_data.concatenate(valid_data)\n",
    "lengths = []\n",
    "for f in os.listdir(os.path.join(train_dir_path, 'healthy')):\n",
    "    tensor_wave = load_wav_16k_mono(os.path.join(train_dir_path, 'healthy', f))\n",
    "    lengths.append(len(tensor_wave))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lengths\n",
    "train_length = tf.data.experimental.cardinality(train_data).numpy()\n",
    "#valid_length = tf.data.experimental.cardinality(valid_data).numpy()\n",
    "\n",
    "# Print the lengths\n",
    "print(\"Length of train_data:\", train_length)\n",
    "#print(\"Length of valid_data:\", valid_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:33.716180Z",
     "iopub.status.busy": "2024-03-23T17:00:33.715809Z",
     "iopub.status.idle": "2024-03-23T17:00:33.730637Z",
     "shell.execute_reply": "2024-03-23T17:00:33.729827Z",
     "shell.execute_reply.started": "2024-03-23T17:00:33.716150Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess(file_path, label):\n",
    "    wav = load_wav_16k_mono(file_path)\n",
    "    wav = wav[:10000]\n",
    "    wav = wav / tf.reduce_max(tf.abs(wav))\n",
    "    zero_padding = tf.zeros([10000] - tf.shape(wav), dtype=tf.float32)\n",
    "    wav = tf.concat([zero_padding, wav],0)\n",
    "    spectrogram = tf.signal.stft(wav, frame_length=80, frame_step=40)\n",
    "    print('Spectrogram Shape: ', spectrogram.shape,type(spectrogram))\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    print('Spectrogram Shape: ', spectrogram.shape)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    print('Spectrogram Shape: ', spectrogram.shape,type(spectrogram))\n",
    "    return spectrogram, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 6\n",
    "Q = 1\n",
    "T = 30000\n",
    "scatt = Scattering1D(J, T, Q)\n",
    "def extract_WST(file_path, label, wav_length=30000):\n",
    "    def scattering_transform(wav):\n",
    "        # Ensure wav is a numpy array\n",
    "        wav = wav.numpy()\n",
    "        \n",
    "        meta = scatt.meta()\n",
    "        order0 = np.where(meta['order'] == 0)\n",
    "        order1 = np.where(meta['order'] == 1)\n",
    "        order2 = np.where(meta['order'] == 2)\n",
    "\n",
    "        Sx = scatt(wav) \n",
    "        return Sx[order1]\n",
    "\n",
    "    wav = load_wav_16k_mono(file_path)\n",
    "    wav = tf.cast(wav, dtype=tf.float32)  # Ensure wav is float32\n",
    "    wav = wav / tf.reduce_max(tf.abs(wav))\n",
    "    wav = wav[:wav_length] if tf.shape(wav)[0] > wav_length else tf.pad(wav, [(0, wav_length - tf.shape(wav)[0])], \"CONSTANT\")\n",
    "\n",
    "    # Using tf.py_function to wrap the scattering transform\n",
    "    scattering_transform = tf.py_function(scattering_transform, [wav], tf.float32)\n",
    "    scattering_transform = tf.abs(scattering_transform)\n",
    "    # You might need to set the shape of the output manually if required\n",
    "    scattering_transform.set_shape((7, 469))  # Set the correct shape based on your scattering output\n",
    "    scattering_transform = tf.expand_dims(scattering_transform, axis=2)\n",
    "    print('scattering_transform',scattering_transform.shape, type(scattering_transform))\n",
    "    return scattering_transform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:37.389949Z",
     "iopub.status.busy": "2024-03-23T17:00:37.389564Z",
     "iopub.status.idle": "2024-03-23T17:00:37.409052Z",
     "shell.execute_reply": "2024-03-23T17:00:37.408118Z",
     "shell.execute_reply.started": "2024-03-23T17:00:37.389918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "iterator = hhl_train.shuffle(buffer_size=10000).as_numpy_iterator()\n",
    "iterator.next()\n",
    "filepath, label = next(iterator)\n",
    "print(filepath, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:40.307339Z",
     "iopub.status.busy": "2024-03-23T17:00:40.306396Z",
     "iopub.status.idle": "2024-03-23T17:00:40.327641Z",
     "shell.execute_reply": "2024-03-23T17:00:40.326773Z",
     "shell.execute_reply.started": "2024-03-23T17:00:40.307294Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "spectrogram, label = preprocess(filepath, label)\n",
    "scattering_coeff, label = extract_WST(filepath, label)\n",
    "print(scattering_coeff)\n",
    "print(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:43.503373Z",
     "iopub.status.busy": "2024-03-23T17:00:43.502676Z",
     "iopub.status.idle": "2024-03-23T17:00:43.652508Z",
     "shell.execute_reply": "2024-03-23T17:00:43.651723Z",
     "shell.execute_reply.started": "2024-03-23T17:00:43.503340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = train_data.map(preprocess)\n",
    "train_data = train_data.cache()\n",
    "train_data = train_data.shuffle(buffer_size=1000)\n",
    "train_data = train_data.batch(4)\n",
    "train_data = train_data.prefetch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:45.895416Z",
     "iopub.status.busy": "2024-03-23T17:00:45.894979Z",
     "iopub.status.idle": "2024-03-23T17:00:45.901683Z",
     "shell.execute_reply": "2024-03-23T17:00:45.900608Z",
     "shell.execute_reply.started": "2024-03-23T17:00:45.895366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_samples = tf.data.experimental.cardinality(train_data).numpy()\n",
    "print(f\"Number of samples in train_data: {num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:48.265552Z",
     "iopub.status.busy": "2024-03-23T17:00:48.265187Z",
     "iopub.status.idle": "2024-03-23T17:00:49.977083Z",
     "shell.execute_reply": "2024-03-23T17:00:49.976092Z",
     "shell.execute_reply.started": "2024-03-23T17:00:48.265524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assuming train_data is your complete dataset\n",
    "total_samples = len(list(train_data.as_numpy_iterator()))  # Get the total number of samples in the dataset\n",
    "\n",
    "# Define split proportions\n",
    "train_size = int(total_samples * 0.7)\n",
    "val_size = int(total_samples * 0.20)\n",
    "test_size = total_samples - train_size - val_size  # Ensures all data is used and accounts for rounding\n",
    "\n",
    "# Create the datasets\n",
    "train = train_data.take(train_size)\n",
    "val = train_data.skip(train_size).take(val_size)\n",
    "test = train_data.skip(train_size + val_size)\n",
    "\n",
    "# Example of extracting a batch from the train dataset to check shapes\n",
    "samples, labels = next(iter(train.batch(1)))\n",
    "print(\"Sample shape:\", samples.shape)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "# This gives you three datasets: train, val, and test\n",
    "# You can iterate over these datasets as needed for training and evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lengths\n",
    "train_length = tf.data.experimental.cardinality(train).numpy()\n",
    "val_length = tf.data.experimental.cardinality(val).numpy()\n",
    "test_length = tf.data.experimental.cardinality(test).numpy()\n",
    "\n",
    "# Print the lengths\n",
    "print(\"Length of train:\", train_length)\n",
    "print(\"Length of val:\", val_length)\n",
    "print(\"Length of test:\", test_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:52.926719Z",
     "iopub.status.busy": "2024-03-23T17:00:52.926348Z",
     "iopub.status.idle": "2024-03-23T17:00:52.932647Z",
     "shell.execute_reply": "2024-03-23T17:00:52.931707Z",
     "shell.execute_reply.started": "2024-03-23T17:00:52.926688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cnn():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (2,2), activation='relu', input_shape=(249, 65, 1)))\n",
    "    model.add(Conv2D(32, (2,2), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:00:56.002446Z",
     "iopub.status.busy": "2024-03-23T17:00:56.002110Z",
     "iopub.status.idle": "2024-03-23T17:00:56.078876Z",
     "shell.execute_reply": "2024-03-23T17:00:56.077938Z",
     "shell.execute_reply.started": "2024-03-23T17:00:56.002421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create an optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model0 = cnn()\n",
    "model0.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T17:01:02.291482Z",
     "iopub.status.busy": "2024-03-23T17:01:02.290658Z",
     "iopub.status.idle": "2024-03-23T17:07:13.791505Z",
     "shell.execute_reply": "2024-03-23T17:07:13.790529Z",
     "shell.execute_reply.started": "2024-03-23T17:01:02.291450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "history0 = model0.fit(train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=50,\n",
    "                    validation_data=test,\n",
    "                    verbose=1,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model0.save(\"Model-cnn-dummy.h5\")\n",
    "print('Model save to Disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history0.history) \n",
    "#df.to_excel(\"output.xlsx\")\n",
    "#df.to_csv(\"output.csv\")\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T07:55:03.323226Z",
     "iopub.status.busy": "2024-03-23T07:55:03.322842Z",
     "iopub.status.idle": "2024-03-23T07:55:03.989905Z",
     "shell.execute_reply": "2024-03-23T07:55:03.988848Z",
     "shell.execute_reply.started": "2024-03-23T07:55:03.323190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Interpreting the Metrics\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss' ]):\n",
    "    ax[i].plot(history0.history[met])\n",
    "    ax[i].plot(history0.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])\n",
    "plt.savefig(\"Model0-Results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-23T07:55:11.521064Z",
     "iopub.status.busy": "2024-03-23T07:55:11.520683Z",
     "iopub.status.idle": "2024-03-23T07:55:12.611305Z",
     "shell.execute_reply": "2024-03-23T07:55:12.610398Z",
     "shell.execute_reply.started": "2024-03-23T07:55:11.521013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results0 = model0.evaluate(test)\n",
    "print(\"test loss, test acc:\", results0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 4: Make predictions and evaluate on the test set\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for samples, labels in test.as_numpy_iterator():\n",
    "    predictions = model0.predict(samples)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)  # Adjust threshold as needed\n",
    "    true_labels.extend(labels)\n",
    "    predicted_labels.extend(predicted_classes)\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Step 5: Classification report and confusion matrix\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model architecture\n",
    "model2 = models.Sequential([\n",
    "    # Since the input includes a singleton dimension, you can use a Lambda layer to remove it\n",
    "    layers.Lambda(lambda x: tf.squeeze(x, axis=-1), input_shape=(249, 65, 1)),\n",
    "    # First BiLSTM layer, return sequences to pass to another LSTM layer\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "    # Second BiLSTM layer, no need to return sequences as this is the final LSTM layer\n",
    "    layers.Bidirectional(layers.LSTM(32)),\n",
    "    # Dense layer for interpretation\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Dropout for regularization\n",
    "    layers.Dropout(0.5),\n",
    "    # Output layer\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model2.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history2 = model2.fit(train_data, epochs=50, verbose=1, validation_data=val, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(\"Model-bilstm-dummy.h5\")\n",
    "print('Model save to Disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history2.history) \n",
    "#df.to_excel(\"output.xlsx\")\n",
    "#df.to_csv(\"output.csv\")\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting the Metrics\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss' ]):\n",
    "    ax[i].plot(history2.history[met])\n",
    "    ax[i].plot(history2.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])\n",
    "plt.savefig(\"Model2-Results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results0 = model2.evaluate(test)\n",
    "print(\"test loss, test acc:\", results0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 4: Make predictions and evaluate on the test set\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for samples, labels in test.as_numpy_iterator():\n",
    "    predictions = model2.predict(samples)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)  # Adjust threshold as needed\n",
    "    true_labels.extend(labels)\n",
    "    predicted_labels.extend(predicted_classes)\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Step 5: Classification report and confusion matrix\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Define the model architecture\n",
    "model3 = models.Sequential([\n",
    "    # Since the input includes a singleton dimension, you can use a Lambda layer to remove it\n",
    "    layers.Lambda(lambda x: tf.squeeze(x, axis=-1), input_shape=(249, 65, 1)),\n",
    "    # First BiLSTM layer, return sequences to pass to another LSTM layer\n",
    "    layers.Bidirectional(layers.SimpleRNN(64, return_sequences=True)),\n",
    "    # Second BiLSTM layer, no need to return sequences as this is the final LSTM layer\n",
    "    layers.Bidirectional(layers.SimpleRNN(32)),\n",
    "    # Dense layer for interpretation\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    # Dropout for regularization\n",
    "    layers.Dropout(0.5),\n",
    "    # Output layer\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer with the learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model3.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history3 = model3.fit(train_data, epochs=30, verbose=1, validation_data=val, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save(\"Model-birnn-dummy.h5\")\n",
    "print('Model save to Disk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history3.history) \n",
    "#df.to_excel(\"output.xlsx\")\n",
    "#df.to_csv(\"output.csv\")\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting the Metrics\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "for i, met in enumerate(['accuracy', 'loss' ]):\n",
    "    ax[i].plot(history3.history[met])\n",
    "    ax[i].plot(history3.history['val_' + met])\n",
    "    ax[i].set_title('Model {}'.format(met))\n",
    "    ax[i].set_xlabel('epochs')\n",
    "    ax[i].set_ylabel(met)\n",
    "    ax[i].legend(['train', 'val'])\n",
    "plt.savefig(\"Model3-Results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "print(\"Evaluate on test data\")\n",
    "results0 = model3.evaluate(test)\n",
    "print(\"test loss, test acc:\", results0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 4: Make predictions and evaluate on the test set\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for samples, labels in test.as_numpy_iterator():\n",
    "    predictions = model3.predict(samples)\n",
    "    predicted_classes = (predictions > 0.5).astype(int)  # Adjust threshold as needed\n",
    "    true_labels.extend(labels)\n",
    "    predicted_labels.extend(predicted_classes)\n",
    "\n",
    "true_labels = np.array(true_labels)\n",
    "predicted_labels = np.array(predicted_labels)\n",
    "\n",
    "# Step 5: Classification report and confusion matrix\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "print(cm)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_dataset(dataset):\n",
    "    # Flatten each sample and collect them\n",
    "    all_samples = []\n",
    "    all_labels = []\n",
    "\n",
    "    for samples, labels in dataset.as_numpy_iterator():\n",
    "        # Flatten from (4, 7, 469, 1) to (4, 7*469)\n",
    "        samples_flattened = samples.reshape(samples.shape[0], -1)\n",
    "        all_samples.append(samples_flattened)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_samples = np.vstack(all_samples)  # Stack arrays vertically\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    return all_samples, all_labels\n",
    "\n",
    "# Prepare the full dataset into features and labels\n",
    "X, y = preprocess_dataset(train_data)  # Assuming train_data contains both train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=48)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "\n",
    "# Create an SVM model with a pipeline that includes scaling\n",
    "model_svm = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    svm.SVC(kernel='linear', random_state=84)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model_svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = model_svm.score(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Predict with the model\n",
    "y_pred = model_svm.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the metrics\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 768212,
     "sourceId": 1324391,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
